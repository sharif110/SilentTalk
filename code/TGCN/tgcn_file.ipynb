{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0b91e6",
   "metadata": {},
   "source": [
    "# videotransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numbers\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "    \n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop the given video sequences (t x h x w) at a random location.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(img, output_size):\n",
    "        \"\"\"Get parameters for ``crop`` for a random crop.\n",
    "        Args:\n",
    "            img (cv2 Image): Image to be cropped.\n",
    "            output_size (tuple): Expected output size of the crop.\n",
    "        Returns:\n",
    "            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n",
    "        \"\"\"\n",
    "        t, c, h, w = img.shape\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = random.randint(0, h - th) if h!=th else 0\n",
    "        j = random.randint(0, w - tw) if w!=tw else 0\n",
    "        return i, j, th, tw\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \n",
    "        i, j, h, w = self.get_params(imgs, self.size)\n",
    "\n",
    "        imgs = imgs[:, :, i:i+h, j:j+w]\n",
    "        return imgs\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(size={0})'.format(self.size)\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = torch.FloatTensor(mean)\n",
    "        self.std = torch.FloatTensor(std)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (T, C, H, W) to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "        \"\"\"\n",
    "        return tensor.sub_(self.mean).div_(self.std)\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    \"\"\"Crops the given seq Images at the center.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (cv2 Image): Image to be cropped.\n",
    "        Returns:\n",
    "            cv2 Image: Cropped image.\n",
    "        \"\"\"\n",
    "        t, c, h, w = imgs.shape\n",
    "        th, tw = self.size\n",
    "        i = int(np.round((h - th) / 2.))\n",
    "        j = int(np.round((w - tw) / 2.))\n",
    "\n",
    "        return imgs[:, :, i:i+th, j:j+tw]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(size={0})'.format(self.size)\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given seq Images randomly with a given probability.\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (seq cv2 Images): seq Images to be flipped.\n",
    "        Returns:\n",
    "            seq Images: Randomly flipped seq images.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            # t x c x h x w\n",
    "            return torch.from_numpy(np.flip(imgs, axis=3).copy())\n",
    "        return imgs\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a8011",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------- label conversion tools ------------------ ##\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "def labels2cat(label_encoder, list):\n",
    "    return label_encoder.transform(list)\n",
    "\n",
    "\n",
    "def labels2onehot(onehot_encoder, label_encoder, labels):\n",
    "    return onehot_encoder.transform(label_encoder.transform(labels).reshape(-1, 1)).toarray()\n",
    "\n",
    "\n",
    "def onehot2labels(label_encoder, y_onehot):\n",
    "    return label_encoder.inverse_transform(np.where(y_onehot == 1)[1]).tolist()\n",
    "\n",
    "\n",
    "def cat2labels(label_encoder, y_cat):\n",
    "    return label_encoder.inverse_transform(y_cat).tolist()\n",
    "\n",
    "\n",
    "# ------------------ RNN utils ------------------------##\n",
    "def init_gru(gru):\n",
    "    if isinstance(gru, nn.GRU):\n",
    "        for param in gru.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "    elif isinstance(gru, nn.GRUCell):\n",
    "        for param in gru.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                init.orthogonal_(param.data)\n",
    "            else:\n",
    "                init.normal_(param.data)\n",
    "\n",
    "\n",
    "def pad_and_pack_sequence(input_sequence):\n",
    "    # pad sequences to have same length\n",
    "    input_sequence = nn.utils.rnn.pad_sequence(input_sequence, batch_first=True)\n",
    "    # calculate lengths of sequences and **store in a tensor**, otherwise pytorch cannot trace correctly.\n",
    "    seq_lengths = torch.LongTensor(list(map(len, input_sequence)))\n",
    "    # create packed sequence        \n",
    "    packed_input_sequence = nn.utils.rnn.pack_padded_sequence(input_sequence, seq_lengths, batch_first=True,\n",
    "                                                              enforce_sorted=False)\n",
    "\n",
    "    return packed_input_sequence\n",
    "\n",
    "\n",
    "def batch_select_tail(batch, in_lengths):\n",
    "    \"\"\" Select tensors from a batch based on the time indices.\n",
    "    \n",
    "    E.g.     \n",
    "    batch = tensor([[[ 0,  1,  2,  3],\n",
    "                     [ 4,  5,  6,  7],\n",
    "                     [ 8,  9, 10, 11]],\n",
    "\n",
    "                     [[12, 13, 14, 15],\n",
    "                     [16, 17, 18, 19],\n",
    "                     [20, 21, 22, 23]]])\n",
    "    of size = (2, 3, 4)\n",
    "    \n",
    "    indices = tensor([1, 2])\n",
    "    \n",
    "    returns tensor([[4, 5, 6, 7],\n",
    "                    [20, 21, 22, 23]])\n",
    "    \"\"\"\n",
    "    rv = torch.stack([torch.index_select(batch[i], 0, in_lengths[i] - 1).squeeze(0) for i in range(batch.size(0))])\n",
    "\n",
    "    return rv\n",
    "\n",
    "\n",
    "def batch_mean_pooling(batch, in_lengths):\n",
    "    \"\"\" Select tensors from a batch based on the input sequence lengths. And apply mean pooling over it.\n",
    "\n",
    "    E.g.\n",
    "    batch = tensor([[[ 0,  1,  2,  3],\n",
    "                     [ 4,  5,  6,  7],\n",
    "                     [ 8,  9, 10, 11]],\n",
    "\n",
    "                     [[12, 13, 14, 15],\n",
    "                     [16, 17, 18, 19],\n",
    "                     [20, 21, 22, 23]]])\n",
    "    of size = (2, 3, 4)\n",
    "\n",
    "    indices = tensor([1, 2])\n",
    "\n",
    "    returns tensor([[0, 1, 2, 3],\n",
    "                    [14, 15, 16, 17]])\n",
    "    \"\"\"\n",
    "    mean = []\n",
    "\n",
    "    for idx, instance in enumerate(batch):\n",
    "        keep = instance[:int(in_lengths[idx])]\n",
    "\n",
    "        mean.append(torch.mean(keep, dim=0))\n",
    "\n",
    "    mean = torch.stack(mean, dim=0)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def gather_last(batch_hidden_states, in_lengths):\n",
    "    num_hidden_states = int(batch_hidden_states.size(2))\n",
    "\n",
    "    indices = in_lengths.unsqueeze(1).unsqueeze(1) - 1\n",
    "    indices = indices.repeat(1, 1, num_hidden_states)\n",
    "\n",
    "    return torch.gather(batch_hidden_states, 1, indices).squeeze(1)\n",
    "\n",
    "\n",
    "# --------------- plotting utils -------------------- #\n",
    "\n",
    "def plot_curves(A=None, B=None, C=None, D=None):\n",
    "    if not A:\n",
    "        A = np.load('output/epoch_training_losses.npy')\n",
    "        B = np.load('output/epoch_training_scores.npy')\n",
    "        C = np.load('output/epoch_test_loss.npy')\n",
    "        D = np.load('output/epoch_test_score.npy')\n",
    "\n",
    "    epochs = A.shape[0]\n",
    "    # plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(np.arange(1, epochs + 1), np.mean(A, axis=1))  # train loss (on epoch end)\n",
    "    plt.plot(np.arange(1, epochs + 1), C)  # test loss (on epoch end)\n",
    "    plt.title(\"model loss\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "\n",
    "    # 2nd figure\n",
    "    plt.subplot(122)\n",
    "    plt.plot(np.arange(1, epochs + 1), np.mean(B, axis=1))  # train accuracy (on epoch end)\n",
    "    plt.plot(np.arange(1, epochs + 1), D)  # test accuracy (on epoch end)\n",
    "    plt.title(\"training scores\")\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "    title = \"output/curves.png\"\n",
    "    plt.savefig(title, dpi=600)\n",
    "    # plt.close(fig)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          save_to=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    # classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label',\n",
    "           )\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=5)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    # fmt = '.2f' if normalize else 'd'\n",
    "    # thresh = cm.max() / 2.\n",
    "    # for i in range(cm.shape[0]):\n",
    "    #     for j in range(cm.shape[1]):\n",
    "    #         ax.text(j, i, format(cm[i, j], fmt),\n",
    "    #                 ha=\"center\", va=\"center\",\n",
    "    #                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "    #                 fontdict={'weight': 'bold', 'size': 5})\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_to:\n",
    "        plt.savefig(save_to, dpi=600)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"../data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # batch_hidden_states = torch.FloatTensor([[[0, 1, 2, 3],\n",
    "    #                            [4, 5, 6, 7],\n",
    "    #                            [8, 9, 10, 11]],\n",
    "    #\n",
    "    #                           [[12, 13, 14, 15],\n",
    "    #                            [16, 17, 18, 19],\n",
    "    #                            [20, 21, 22, 23]]])\n",
    "\n",
    "    t = torch.Tensor([1, 2, 3])\n",
    "\n",
    "    mask_indices = [0, 2]\n",
    "    mask_indices = torch.LongTensor(mask_indices)\n",
    "\n",
    "    # frame_preds = torch.index_select(frame_preds, dim=0, index=mask_indices)\n",
    "    gt = torch.index_select(t, dim=0, index=mask_indices)\n",
    "\n",
    "    print(gt)\n",
    "\n",
    "    # # batch_lengths = torch.ones(size=(4, 20))\n",
    "    #\n",
    "    # fc2 = nn.Linear(4, 20)\n",
    "    # import torch.nn.functional as F\n",
    "    # loss = F.cross_entropy(batch_hidden_states[0], torch.LongTensor([[0, 1, 2], [0, 1, 2]]))\n",
    "    # print(loss)\n",
    "    # print(mean_pooling(batch_hidden_states, batch_lengths))\n",
    "    # print(batch_mean_pooling(batch_hidden_states, batch_lengths))\n",
    "    # print(fc2(batch_hidden_states).size())\n",
    "    #\n",
    "    # hidden_x_dirs = int(batch_hidden_states.size(2))\n",
    "    #\n",
    "    # indices = batch_lengths.unsqueeze(1).unsqueeze(1) - 1\n",
    "    # indices = indices.repeat(1, 1, hidden_x_dirs)\n",
    "    #\n",
    "    # last_hidden_out = torch.gather(batch_hidden_states, 1, indices).squeeze(1)\n",
    "    #\n",
    "    # print(last_hidden_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fbf37",
   "metadata": {},
   "source": [
    "# train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ba276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(log_interval, model, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    losses = []\n",
    "    scores = []\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "\n",
    "    N_count = 0  # counting total trained sample in one epoch\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        X, y, video_ids = data\n",
    "        # distribute data to device\n",
    "        X, y = X.cuda(), y.cuda().view(-1).long()  # Cast labels to 'Long'\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(X)  # output has dim = (batch, number of classes)\n",
    "\n",
    "        loss = compute_loss(out, y)\n",
    "\n",
    "        # loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "        y_pred = torch.max(out, 1)[1]  # y_pred != output\n",
    "\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "        # collect prediction labels\n",
    "        train_labels.extend(y.cpu().data.squeeze().tolist())\n",
    "        train_preds.extend(y_pred.cpu().data.squeeze().tolist())\n",
    "\n",
    "        scores.append(step_score)  # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=6)\n",
    "        #\n",
    "        # for p in model.parameters():\n",
    "        #     param_norm = p.grad.data.norm(2)\n",
    "        #     total_norm += param_norm.item() ** 2\n",
    "        # total_norm = total_norm ** (1. / 2)\n",
    "        #\n",
    "        # print(total_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.6f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(),\n",
    "                100 * step_score))\n",
    "\n",
    "    return losses, scores, train_labels, train_preds\n",
    "\n",
    "\n",
    "def validation(model, test_loader, epoch, save_to):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = []\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    all_video_ids = []\n",
    "    all_pool_out = []\n",
    "\n",
    "    num_copies = 4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            # distribute data to device\n",
    "            X, y, video_ids = data\n",
    "            X, y = X.cuda(), y.cuda().view(-1, ).long()\n",
    "\n",
    "            all_output = []\n",
    "\n",
    "            stride = X.size()[2] // num_copies\n",
    "\n",
    "            for i in range(num_copies):\n",
    "                X_slice = X[:, :, i * stride: (i+1) * stride]\n",
    "                output = model(X_slice)\n",
    "                all_output.append(output)\n",
    "\n",
    "            all_output = torch.stack(all_output, dim=1)\n",
    "            output = torch.mean(all_output, dim=1)\n",
    "\n",
    "            # output = model(X)  # output has dim = (batch, number of classes)\n",
    "\n",
    "            # loss = F.cross_entropy(pool_out, y, reduction='sum')\n",
    "            loss = compute_loss(output, y)\n",
    "\n",
    "            val_loss.append(loss.item())  # sum up batch loss\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "            all_video_ids.extend(video_ids)\n",
    "            all_pool_out.extend(output)\n",
    "\n",
    "    # this computes the average loss on the BATCH\n",
    "    val_loss = sum(val_loss) / len(val_loss)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0).squeeze()\n",
    "    all_pool_out = torch.stack(all_pool_out, dim=0).cpu().data.numpy()\n",
    "\n",
    "    # log down incorrectly labelled instances\n",
    "    incorrect_indices = torch.nonzero(all_y - all_y_pred).squeeze().data\n",
    "    incorrect_video_ids = [(vid, int(all_y_pred[i].data)) for i, vid in enumerate(all_video_ids) if\n",
    "                           i in incorrect_indices]\n",
    "\n",
    "    all_y = all_y.cpu().data.numpy()\n",
    "    all_y_pred = all_y_pred.cpu().data.numpy()\n",
    "\n",
    "    # top-k accuracy\n",
    "    top1acc = accuracy_score(all_y, all_y_pred)\n",
    "    top3acc = compute_top_n_accuracy(all_y, all_pool_out, 3)\n",
    "    top5acc = compute_top_n_accuracy(all_y, all_pool_out, 5)\n",
    "    top10acc = compute_top_n_accuracy(all_y, all_pool_out, 10)\n",
    "    top30acc = compute_top_n_accuracy(all_y, all_pool_out, 30)\n",
    "\n",
    "    # show information\n",
    "    print('\\nVal. set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), val_loss,\n",
    "                                                                                        100 * top1acc))\n",
    "\n",
    "    if save_to:\n",
    "        # save Pytorch models of best record\n",
    "        torch.save(model.state_dict(),\n",
    "                   os.path.join(save_to, 'gcn_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "        print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return val_loss, [top1acc, top3acc, top5acc, top10acc, top30acc], all_y.tolist(), all_y_pred.tolist(), incorrect_video_ids\n",
    "\n",
    "\n",
    "def compute_loss(out, gt):\n",
    "    gt = gt.long()  # Convert labels to 'Long' data type\n",
    "    ce_loss = F.cross_entropy(out, gt)\n",
    "    return ce_loss\n",
    "\n",
    "\n",
    "def compute_top_n_accuracy(truths, preds, n):\n",
    "    best_n = np.argsort(preds, axis=1)[:, -n:]\n",
    "    ts = truths\n",
    "    successes = 0\n",
    "    for i in range(ts.shape[0]):\n",
    "        if ts[i] in best_n[i, :]:\n",
    "            successes += 1\n",
    "    return float(successes) / ts.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f8668",
   "metadata": {},
   "source": [
    "# train_tcgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d6408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import utils\n",
    "from configs import Config\n",
    "from tgcn_model import GCN_muti_att\n",
    "from sign_dataset import Sign_Dataset\n",
    "from train_utils import train, validation\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "\n",
    "def run(split_file, pose_data_root, configs, save_model_to=None):\n",
    "    epochs = configs.max_epochs\n",
    "    log_interval = configs.log_interval\n",
    "    num_samples = configs.num_samples\n",
    "    hidden_size = configs.hidden_size\n",
    "    drop_p = configs.drop_p\n",
    "    num_stages = configs.num_stages\n",
    "\n",
    "    # setup dataset\n",
    "    train_dataset = Sign_Dataset(index_file_path=split_file, split=['train', 'val'], pose_root=pose_data_root,\n",
    "                                 img_transforms=None, video_transforms=None, num_samples=num_samples)\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=configs.batch_size,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "    val_dataset = Sign_Dataset(index_file_path=split_file, split='test', pose_root=pose_data_root,\n",
    "                               img_transforms=None, video_transforms=None,\n",
    "                               num_samples=num_samples,\n",
    "                               sample_strategy='k_copies')\n",
    "    val_data_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=configs.batch_size,\n",
    "                                                  shuffle=True)\n",
    "\n",
    "    logging.info('\\n'.join(['Class labels are: '] + [(str(i) + ' - ' + label) for i, label in\n",
    "                                                     enumerate(train_dataset.label_encoder.classes_)]))\n",
    "\n",
    "    # setup the model\n",
    "    model = GCN_muti_att(input_feature=num_samples*2, hidden_feature=num_samples*2,\n",
    "                         num_class=len(train_dataset.label_encoder.classes_), p_dropout=drop_p, num_stage=num_stages).cuda()\n",
    "\n",
    "    # setup training parameters, learning rate, optimizer, scheduler\n",
    "    lr = configs.init_lr\n",
    "    # optimizer = optim.SGD(vgg_gru.parameters(), lr=lr, momentum=0.00001)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, eps=configs.adam_eps, weight_decay=configs.adam_weight_decay)\n",
    "\n",
    "    # record training process\n",
    "    epoch_train_losses = []\n",
    "    epoch_train_scores = []\n",
    "    epoch_val_losses = []\n",
    "    epoch_val_scores = []\n",
    "\n",
    "    best_test_acc = 0\n",
    "    # start training\n",
    "    for epoch in range(int(epochs)):\n",
    "        # train, test model\n",
    "\n",
    "        print('start training.')\n",
    "        train_losses, train_scores, train_gts, train_preds = train(log_interval, model,\n",
    "                                                                   train_data_loader, optimizer, epoch)\n",
    "        print('start testing.')\n",
    "        val_loss, val_score, val_gts, val_preds, incorrect_samples = validation(model,\n",
    "                                                                                val_data_loader, epoch,\n",
    "                                                                                save_to=save_model_to)\n",
    "        # print('start testing.')\n",
    "        # val_loss, val_score, val_gts, val_preds, incorrect_samples = validation(model,\n",
    "        #                                                                         val_data_loader, epoch,\n",
    "        #                                                                         save_to=save_model_to)\n",
    "\n",
    "        logging.info('========================\\nEpoch: {} Average loss: {:.4f}'.format(epoch, val_loss))\n",
    "        logging.info('Top-1 acc: {:.4f}'.format(100 * val_score[0]))\n",
    "        logging.info('Top-3 acc: {:.4f}'.format(100 * val_score[1]))\n",
    "        logging.info('Top-5 acc: {:.4f}'.format(100 * val_score[2]))\n",
    "        logging.info('Top-10 acc: {:.4f}'.format(100 * val_score[3]))\n",
    "        logging.info('Top-30 acc: {:.4f}'.format(100 * val_score[4]))\n",
    "        logging.debug('mislabelled val. instances: ' + str(incorrect_samples))\n",
    "\n",
    "        # save results\n",
    "        epoch_train_losses.append(train_losses)\n",
    "        epoch_train_scores.append(train_scores)\n",
    "        epoch_val_losses.append(val_loss)\n",
    "        epoch_val_scores.append(val_score[0])\n",
    "\n",
    "        # save all train test results\n",
    "        np.save('output/epoch_training_losses.npy', np.array(epoch_train_losses))\n",
    "        np.save('output/epoch_training_scores.npy', np.array(epoch_train_scores))\n",
    "        np.save('output/epoch_test_loss.npy', np.array(epoch_val_losses))\n",
    "        np.save('output/epoch_test_score.npy', np.array(epoch_val_scores))\n",
    "\n",
    "        if val_score[0] > best_test_acc:\n",
    "            best_test_acc = val_score[0]\n",
    "            best_epoch_num = epoch\n",
    "\n",
    "            # Define the path to the directory\n",
    "            save_dir = \"checkpoints/asl100\"\n",
    "            # Create the directory if it doesn't exist\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            torch.save(model.state_dict(), os.path.join('checkpoints', subset, 'gcn_epoch={}_val_acc={}.pth'.format(\n",
    "                best_epoch_num, best_test_acc)))\n",
    "\n",
    "    utils.plot_curves()\n",
    "\n",
    "    class_names = train_dataset.label_encoder.classes_\n",
    "    utils.plot_confusion_matrix(train_gts, train_preds, classes=class_names, normalize=False,\n",
    "                                save_to='output/train-conf-mat')\n",
    "    utils.plot_confusion_matrix(val_gts, val_preds, classes=class_names, normalize=False, save_to='output/val-conf-mat')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = 'C:/Users/Pc/Desktop/WLASL/'\n",
    "\n",
    "    subset = 'asl2000'\n",
    "\n",
    "    split_file = os.path.join(root, 'data/splits/{}.json'.format(subset))\n",
    "    pose_data_root = os.path.join(root, 'data/pose_per_individual_videos')\n",
    "    config_file = os.path.join(root, 'code/TGCN/configs/{}.ini'.format(subset))\n",
    "    configs = Config(config_file)\n",
    "\n",
    "    logging.basicConfig(filename='{}.log'.format(os.path.basename(config_file)[:-4]), level=logging.DEBUG, filemode='w+')\n",
    "\n",
    "    logging.info('Calling main.run()')\n",
    "    run(split_file=split_file, configs=configs, pose_data_root=pose_data_root)\n",
    "    logging.info('Finished main.run()')\n",
    "    # utils.plot_curves()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dbdead",
   "metadata": {},
   "source": [
    "# tgcn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e07716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GraphConvolution_att(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, init_A=0):\n",
    "        super(GraphConvolution_att, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.att = Parameter(torch.FloatTensor(55, 55))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.att.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # AHW\n",
    "        support = torch.matmul(input, self.weight)  # HW\n",
    "        output = torch.matmul(self.att, support)  # g\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GC_Block(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, p_dropout, bias=True, is_resi=True):\n",
    "        super(GC_Block, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = in_features\n",
    "        self.is_resi = is_resi\n",
    "\n",
    "        self.gc1 = GraphConvolution_att(in_features, in_features)\n",
    "        self.bn1 = nn.BatchNorm1d(55 * in_features)\n",
    "\n",
    "        self.gc2 = GraphConvolution_att(in_features, in_features)\n",
    "        self.bn2 = nn.BatchNorm1d(55 * in_features)\n",
    "\n",
    "        self.do = nn.Dropout(p_dropout)\n",
    "        self.act_f = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.gc1(x)\n",
    "        b, n, f = y.shape\n",
    "        y = self.bn1(y.view(b, -1)).view(b, n, f)\n",
    "        y = self.act_f(y)\n",
    "        y = self.do(y)\n",
    "\n",
    "        y = self.gc2(y)\n",
    "        b, n, f = y.shape\n",
    "        y = self.bn2(y.view(b, -1)).view(b, n, f)\n",
    "        y = self.act_f(y)\n",
    "        y = self.do(y)\n",
    "        if self.is_resi:\n",
    "            return y + x\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GCN_muti_att(nn.Module):\n",
    "    def __init__(self, input_feature, hidden_feature, num_class, p_dropout, num_stage=1, is_resi=True):\n",
    "        super(GCN_muti_att, self).__init__()\n",
    "        self.num_stage = num_stage\n",
    "\n",
    "        self.gc1 = GraphConvolution_att(input_feature, hidden_feature)\n",
    "        self.bn1 = nn.BatchNorm1d(55 * hidden_feature)\n",
    "\n",
    "        self.gcbs = []\n",
    "        for i in range(num_stage):\n",
    "            self.gcbs.append(GC_Block(hidden_feature, p_dropout=p_dropout, is_resi=is_resi))\n",
    "\n",
    "        self.gcbs = nn.ModuleList(self.gcbs)\n",
    "\n",
    "        # self.gc7 = GraphConvolution_att(hidden_feature, output_feature)\n",
    "\n",
    "        self.do = nn.Dropout(p_dropout)\n",
    "        self.act_f = nn.Tanh()\n",
    "\n",
    "        # self.fc1 = nn.Linear(55 * output_feature, fc1_out)\n",
    "        self.fc_out = nn.Linear(hidden_feature, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.gc1(x)\n",
    "        b, n, f = y.shape\n",
    "        y = self.bn1(y.view(b, -1)).view(b, n, f)\n",
    "        y = self.act_f(y)\n",
    "        y = self.do(y)\n",
    "\n",
    "        for i in range(self.num_stage):\n",
    "            y = self.gcbs[i](y)\n",
    "\n",
    "        # y = self.gc7(y)\n",
    "        out = torch.mean(y, dim=1)\n",
    "        out = self.fc_out(out)\n",
    "\n",
    "        # print(\"out::::::\",out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_samples = 32\n",
    "\n",
    "    model = GCN_muti_att(input_feature=num_samples*2, hidden_feature=256,\n",
    "                         num_class=100, p_dropout=0.3, num_stage=2)\n",
    "    x = torch.ones([2, 55, num_samples*2])\n",
    "    print(model(x).size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71590e62",
   "metadata": {},
   "source": [
    "# test_tgcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47104645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from configs import Config\n",
    "from sign_dataset import Sign_Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tgcn_model import GCN_muti_att\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = []\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    all_video_ids = []\n",
    "    all_pool_out = []\n",
    "\n",
    "    num_copies = 4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            print('starting batch: {}'.format(batch_idx))\n",
    "            # distribute data to device\n",
    "            X, y, video_ids,gloss = data\n",
    "            X, y = X.cuda(), y.cuda().view(-1, ).long()\n",
    "\n",
    "            all_output = []\n",
    "\n",
    "            stride = X.size()[2] // num_copies\n",
    "\n",
    "            for i in range(num_copies):\n",
    "                X_slice = X[:, :, i * stride: (i + 1) * stride]\n",
    "                output = model(X_slice)\n",
    "                all_output.append(output)\n",
    "\n",
    "            all_output = torch.stack(all_output, dim=1)\n",
    "            output = torch.mean(all_output, dim=1)\n",
    "\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "            all_video_ids.extend(video_ids)\n",
    "            all_pool_out.extend(output)\n",
    "    print(all_y_pred)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0).squeeze()\n",
    "    all_pool_out = torch.stack(all_pool_out, dim=0).cpu().data.numpy()\n",
    "\n",
    "    # log down incorrectly labelled instances\n",
    "    incorrect_indices = torch.nonzero(all_y - all_y_pred).squeeze().data\n",
    "    incorrect_video_ids = [(vid, int(all_y_pred[i].data)) for i, vid in enumerate(all_video_ids) if\n",
    "                           i in incorrect_indices]\n",
    "\n",
    "    all_y = all_y.cpu().data.numpy()\n",
    "    all_y_pred = all_y_pred.cpu().data.numpy()\n",
    "\n",
    "    # top-k accuracy\n",
    "    top1acc = accuracy_score(all_y, all_y_pred)\n",
    "    top3acc = compute_top_n_accuracy(all_y, all_pool_out, 3)\n",
    "    top5acc = compute_top_n_accuracy(all_y, all_pool_out, 5)\n",
    "    top10acc = compute_top_n_accuracy(all_y, all_pool_out, 10)\n",
    "    top30acc = compute_top_n_accuracy(all_y, all_pool_out, 30)\n",
    "\n",
    "    # show information\n",
    "    print('\\nVal. set ({:d} samples): top-1 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top1acc))\n",
    "    print('\\nVal. set ({:d} samples): top-3 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top3acc))\n",
    "    print('\\nVal. set ({:d} samples): top-5 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top5acc))\n",
    "    print('\\nVal. set ({:d} samples): top-10 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top10acc))\n",
    "\n",
    "\n",
    "def compute_top_n_accuracy(truths, preds, n):\n",
    "    best_n = np.argsort(preds, axis=1)[:, -n:]\n",
    "    ts = truths\n",
    "    successes = 0\n",
    "    for i in range(ts.shape[0]):\n",
    "        if ts[i] in best_n[i, :]:\n",
    "            successes += 1\n",
    "    return float(successes) / ts.shape[0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # change root and subset accordingly.\n",
    "    root = 'C:/Users/Pc/Desktop/WLASL/'\n",
    "    trained_on = 'asl100'\n",
    "\n",
    "    checkpoint = 'ckpt.pth'\n",
    "\n",
    "    split_file = os.path.join(root, 'data/splits/{}.json'.format(trained_on))\n",
    "    # test_on_split_file = os.path.join(root, 'data/splits-with-dialect-annotated/{}.json'.format(tested_on))\n",
    "\n",
    "    pose_data_root = os.path.join(root, 'data/pose_per_individual_videos')\n",
    "    # config_file = os.path.join(root, 'code/TGCN/archived/asl100/asl100.ini')\n",
    "\n",
    "    config_file = os.path.join(root, 'code/TGCN/archived/{}/{}.ini'.format(trained_on, trained_on))\n",
    "    print(config_file)\n",
    "    configs = Config(config_file)\n",
    "\n",
    "    num_samples = configs.num_samples\n",
    "    hidden_size = configs.hidden_size\n",
    "    drop_p = configs.drop_p\n",
    "    num_stages = configs.num_stages\n",
    "    batch_size = configs.batch_size\n",
    "\n",
    "    dataset = Sign_Dataset(index_file_path=split_file, split='test', pose_root=pose_data_root,\n",
    "                           img_transforms=None, video_transforms=None,\n",
    "                           num_samples=num_samples,\n",
    "                           sample_strategy='k_copies',\n",
    "                           test_index_file=split_file\n",
    "                           )\n",
    "    \n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # setup the model\n",
    "    model = GCN_muti_att(input_feature=num_samples * 2, hidden_feature=hidden_size,\n",
    "                         num_class=int(trained_on[3:]), p_dropout=drop_p, num_stage=num_stages).cuda()\n",
    "\n",
    "    print('Loading model...')\n",
    "    checkpoint = torch.load(os.path.join(root, 'code/TGCN/archived/{}/{}'.format(trained_on, checkpoint)))\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print('Finish loading model!')\n",
    "\n",
    "    test(model, data_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d1dc6",
   "metadata": {},
   "source": [
    "# test tgcn 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cabe66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from configs import Config\n",
    "from sign_dataset import Sign_Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import utils\n",
    "\n",
    "from tgcn_model import GCN_muti_att\n",
    "\n",
    "\n",
    "def create_label_mapping(dataset):\n",
    "    # Create a mapping from video ID to gloss category\n",
    "    label_mapping = {}\n",
    "\n",
    "    for item in dataset.data:\n",
    "        video_id = item['video_id']\n",
    "        gloss_cat = item['gloss_cat']\n",
    "        gloss = item['gloss']\n",
    "        # Assuming gloss_cat is the categorical representation\n",
    "        # original_gloss = utils.cat2labels.inverse_transform([gloss_cat])[0]\n",
    "\n",
    "        # print(original_gloss)\n",
    "        label_mapping[gloss_cat] = gloss\n",
    "\n",
    "    return label_mapping\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_loader, label_mapping):\n",
    "    # print(label_mapping)\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = []\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    all_video_ids = []\n",
    "    all_pool_out = []\n",
    "    all_text_predictions = []  # Store text predictions\n",
    "\n",
    "    num_copies = 4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            print('starting batch: {}'.format(batch_idx))\n",
    "            # distribute data to device\n",
    "            X, y, video_ids,gloss = data\n",
    "            X, y = X.cuda(), y.cuda().view(-1, ).long()\n",
    "\n",
    "            all_output = []\n",
    "\n",
    "            stride = X.size()[2] // num_copies\n",
    "\n",
    "            for i in range(num_copies):\n",
    "                X_slice = X[:, :, i * stride: (i + 1) * stride]\n",
    "                output = model(X_slice)\n",
    "                all_output.append(output)\n",
    "\n",
    "            all_output = torch.stack(all_output, dim=1)\n",
    "            output = torch.mean(all_output, dim=1)\n",
    "\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "            all_video_ids.extend(video_ids)\n",
    "            all_pool_out.extend(output)\n",
    "\n",
    "            # print(\"predict\",y_pred)\n",
    "\n",
    "            # Convert numeric predictions to text labels\n",
    "            text_predictions = [label_mapping[pred.item()] for pred in y_pred]\n",
    "            all_text_predictions.extend(text_predictions)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_y = torch.stack(all_y, dim=0).cpu()\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0).cpu().squeeze()\n",
    "    all_pool_out = torch.stack(all_pool_out, dim=0).cpu().data.numpy()\n",
    "\n",
    "    # Calculate accuracy and other metrics\n",
    "    top1acc = accuracy_score(all_y, all_y_pred)\n",
    "    top3acc = compute_top_n_accuracy(all_y, all_pool_out, 3)\n",
    "    top5acc = compute_top_n_accuracy(all_y, all_pool_out, 5)\n",
    "    top10acc = compute_top_n_accuracy(all_y, all_pool_out, 10)\n",
    "    top30acc = compute_top_n_accuracy(all_y, all_pool_out, 30)\n",
    "\n",
    "    # Output metrics\n",
    "    print('\\nVal. set ({:d} samples): top-1 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top1acc))\n",
    "    print('\\nVal. set ({:d} samples): top-3 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top3acc))\n",
    "    print('\\nVal. set ({:d} samples): top-5 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top5acc))\n",
    "    print('\\nVal. set ({:d} samples): top-10 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top10acc))\n",
    "\n",
    "    # Output text predictions\n",
    "    print('Text Predictions:', all_text_predictions)\n",
    "\n",
    "        # Return text predictions\n",
    "    return all_text_predictions\n",
    "\n",
    "\n",
    "# Helper function to compute top-n accuracy\n",
    "def compute_top_n_accuracy(truths, preds, n):\n",
    "    best_n = np.argsort(preds, axis=1)[:, -n:]\n",
    "    ts = truths\n",
    "    successes = 0\n",
    "    for i in range(ts.shape[0]):\n",
    "        if ts[i] in best_n[i, :]:\n",
    "            successes += 1\n",
    "    return float(successes) / ts.shape[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # change root and subset accordingly.\n",
    "    root = 'C:/Users/Pc/Desktop/WLASL/'\n",
    "    trained_on = 'asl100'\n",
    "\n",
    "    checkpoint = 'ckpt.pth'\n",
    "\n",
    "    split_file = os.path.join(root, 'data/splits/{}.json'.format(trained_on))\n",
    "    # test_on_split_file = os.path.join(root, 'data/splits-with-dialect-annotated/{}.json'.format(tested_on))\n",
    "\n",
    "    pose_data_root = os.path.join(root, 'data/pose_per_individual_videos')\n",
    "    # config_file = os.path.join(root, 'code/TGCN/archived/asl100/asl100.ini')\n",
    "\n",
    "    config_file = os.path.join(root, 'code/TGCN/archived/{}/{}.ini'.format(trained_on, trained_on))\n",
    "    print(config_file)\n",
    "    configs = Config(config_file)\n",
    "\n",
    "    num_samples = configs.num_samples\n",
    "    hidden_size = configs.hidden_size\n",
    "    drop_p = configs.drop_p\n",
    "    num_stages = configs.num_stages\n",
    "    batch_size = configs.batch_size\n",
    "\n",
    "    dataset = Sign_Dataset(index_file_path=split_file, split='test', pose_root=pose_data_root,\n",
    "                           img_transforms=None, video_transforms=None,\n",
    "                           num_samples=num_samples,\n",
    "                           sample_strategy='k_copies',\n",
    "                           test_index_file=split_file\n",
    "                           )\n",
    "    \n",
    "    for item in dataset:\n",
    "        print(item)\n",
    "    \n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # setup the model\n",
    "    model = GCN_muti_att(input_feature=num_samples * 2, hidden_feature=hidden_size,\n",
    "                         num_class=int(trained_on[3:]), p_dropout=drop_p, num_stage=num_stages).cuda()\n",
    "\n",
    "    print('Loading model...')\n",
    "\n",
    "    checkpoint = torch.load(os.path.join(root, 'code/TGCN/archived/{}/{}'.format(trained_on, checkpoint)))\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print('Finish loading model!')\n",
    "    \n",
    "    # Create label mapping\n",
    "    label_mapping = create_label_mapping(dataset)\n",
    "\n",
    "    # Test the model\n",
    "    test(model, data_loader, label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b29cd8",
   "metadata": {},
   "source": [
    "# test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from configs import Config\n",
    "from sign_dataset import Sign_Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tgcn_model import GCN_muti_att\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = []\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    all_video_ids = []\n",
    "    all_pool_out = []\n",
    "\n",
    "    num_copies = 4\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            print('starting batch: {}'.format(batch_idx))\n",
    "            # distribute data to device\n",
    "            X, y, video_ids = data\n",
    "            X, y = X.cuda(), y.cuda().view(-1, )\n",
    "\n",
    "            all_output = []\n",
    "\n",
    "            stride = X.size()[2] // num_copies\n",
    "\n",
    "            for i in range(num_copies):\n",
    "                X_slice = X[:, :, i * stride: (i + 1) * stride]\n",
    "                output = model(X_slice)\n",
    "                all_output.append(output)\n",
    "\n",
    "            all_output = torch.stack(all_output, dim=1)\n",
    "            output = torch.mean(all_output, dim=1)\n",
    "\n",
    "            y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "            all_video_ids.extend(video_ids)\n",
    "            all_pool_out.extend(output)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0).squeeze()\n",
    "    all_pool_out = torch.stack(all_pool_out, dim=0).cpu().data.numpy()\n",
    "\n",
    "    # log down incorrectly labelled instances\n",
    "    incorrect_indices = torch.nonzero(all_y - all_y_pred).squeeze().data\n",
    "    incorrect_video_ids = [(vid, int(all_y_pred[i].data)) for i, vid in enumerate(all_video_ids) if\n",
    "                           i in incorrect_indices]\n",
    "\n",
    "    all_y = all_y.cpu().data.numpy()\n",
    "    all_y_pred = all_y_pred.cpu().data.numpy()\n",
    "\n",
    "    # top-k accuracy\n",
    "    top1acc = accuracy_score(all_y, all_y_pred)\n",
    "    top3acc = compute_top_n_accuracy(all_y, all_pool_out, 3)\n",
    "    top5acc = compute_top_n_accuracy(all_y, all_pool_out, 5)\n",
    "    top10acc = compute_top_n_accuracy(all_y, all_pool_out, 10)\n",
    "    top30acc = compute_top_n_accuracy(all_y, all_pool_out, 30)\n",
    "\n",
    "    # show information\n",
    "    print('\\nVal. set ({:d} samples): top-1 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top1acc))\n",
    "    print('\\nVal. set ({:d} samples): top-3 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top3acc))\n",
    "    print('\\nVal. set ({:d} samples): top-5 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top5acc))\n",
    "    print('\\nVal. set ({:d} samples): top-10 Accuracy: {:.2f}%\\n'.format(len(all_y), 100 * top10acc))\n",
    "\n",
    "\n",
    "def compute_top_n_accuracy(truths, preds, n):\n",
    "    best_n = np.argsort(preds, axis=1)[:, -n:]\n",
    "    ts = truths\n",
    "    successes = 0\n",
    "    for i in range(ts.shape[0]):\n",
    "        if ts[i] in best_n[i, :]:\n",
    "            successes += 1\n",
    "    return float(successes) / ts.shape[0]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # change root and subset accordingly.\n",
    "    root = '/media/anudisk/github/WLASL'\n",
    "    trained_on = 'asl100'\n",
    "\n",
    "    checkpoint = 'ckpt.pth'\n",
    "\n",
    "    split_file = os.path.join(root, 'data/splits/{}.json'.format(trained_on))\n",
    "    # test_on_split_file = os.path.join(root, 'data/splits-with-dialect-annotated/{}.json'.format(tested_on))\n",
    "\n",
    "    pose_data_root = os.path.join(root, 'data/pose_per_individual_videos')\n",
    "    config_file = os.path.join(root, 'code/TGCN/archive/{}/{}.ini'.format(trained_on, trained_on))\n",
    "    configs = Config(config_file)\n",
    "\n",
    "    num_samples = configs.num_samples\n",
    "    hidden_size = configs.hidden_size\n",
    "    drop_p = configs.drop_p\n",
    "    num_stages = configs.num_stages\n",
    "    batch_size = configs.batch_size\n",
    "\n",
    "    dataset = Sign_Dataset(index_file_path=split_file, split='test', pose_root=pose_data_root,\n",
    "                           img_transforms=None, video_transforms=None,\n",
    "                           num_samples=num_samples,\n",
    "                           sample_strategy='k_copies',\n",
    "                           test_index_file=split_file\n",
    "                           )\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # setup the model\n",
    "    model = GCN_muti_att(input_feature=num_samples * 2, hidden_feature=hidden_size,\n",
    "                         num_class=int(trained_on[3:]), p_dropout=drop_p, num_stage=num_stages).cuda()\n",
    "\n",
    "    print('Loading model...')\n",
    "\n",
    "    checkpoint = torch.load(os.path.join(root, 'code/TGCN/archive/{}/{}'.format(trained_on, checkpoint)))\n",
    "    model.load_state_dict(checkpoint)\n",
    "    print('Finish loading model!')\n",
    "\n",
    "    test(model, data_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf82ffe",
   "metadata": {},
   "source": [
    "# signdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed9a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import utils\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "\n",
    "def compute_difference(x):\n",
    "    diff = []\n",
    "\n",
    "    for i, xx in enumerate(x):\n",
    "        temp = []\n",
    "        for j, xxx in enumerate(x):\n",
    "            if i != j:\n",
    "                temp.append(xx - xxx)\n",
    "\n",
    "        diff.append(temp)\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def read_pose_file(filepath):\n",
    "    body_pose_exclude = {9, 10, 11, 22, 23, 24, 12, 13, 14, 19, 20, 21}\n",
    "\n",
    "    try:\n",
    "        content = json.load(open(filepath))[\"people\"][0]\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "    path_parts = os.path.split(filepath)\n",
    "\n",
    "    frame_id = path_parts[1][:11]\n",
    "    vid = os.path.split(path_parts[0])[-1]\n",
    "\n",
    "    save_to = os.path.join('C:/Users/Pc/Desktop/WLASL/data/pose_per_individual_videos', vid)\n",
    "\n",
    "    try:\n",
    "        ft = torch.load(os.path.join(save_to, frame_id + '_ft.pt'))\n",
    "\n",
    "        xy = ft[:, :2]\n",
    "        # angles = torch.atan(ft[:, 110:]) / 90\n",
    "        # ft = torch.cat([xy, angles], dim=1)\n",
    "        return xy\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(filepath)\n",
    "        body_pose = content[\"pose_keypoints_2d\"]\n",
    "        left_hand_pose = content[\"hand_left_keypoints_2d\"]\n",
    "        right_hand_pose = content[\"hand_right_keypoints_2d\"]\n",
    "\n",
    "        body_pose.extend(left_hand_pose)\n",
    "        body_pose.extend(right_hand_pose)\n",
    "\n",
    "        x = [v for i, v in enumerate(body_pose) if i % 3 == 0 and i // 3 not in body_pose_exclude]\n",
    "        y = [v for i, v in enumerate(body_pose) if i % 3 == 1 and i // 3 not in body_pose_exclude]\n",
    "        # conf = [v for i, v in enumerate(body_pose) if i % 3 == 2 and i // 3 not in body_pose_exclude]\n",
    "\n",
    "        x = 2 * ((torch.FloatTensor(x) / 256.0) - 0.5)\n",
    "        y = 2 * ((torch.FloatTensor(y) / 256.0) - 0.5)\n",
    "        # conf = torch.FloatTensor(conf)\n",
    "\n",
    "        x_diff = torch.FloatTensor(compute_difference(x)) / 2\n",
    "        y_diff = torch.FloatTensor(compute_difference(y)) / 2\n",
    "\n",
    "        zero_indices = (x_diff == 0).nonzero()\n",
    "\n",
    "        orient = y_diff / x_diff\n",
    "        orient[zero_indices] = 0\n",
    "\n",
    "        xy = torch.stack([x, y]).transpose_(0, 1)\n",
    "\n",
    "        ft = torch.cat([xy, x_diff, y_diff, orient], dim=1)\n",
    "\n",
    "        path_parts = os.path.split(filepath)\n",
    "\n",
    "        frame_id = path_parts[1][:11]\n",
    "        vid = os.path.split(path_parts[0])[-1]\n",
    "\n",
    "        save_to = os.path.join('C:/Users/Pc/Desktop/WLASL/data/pose_per_individual_videos', vid)\n",
    "        if not os.path.exists(save_to):\n",
    "            os.mkdir(save_to)\n",
    "        torch.save(ft, os.path.join(save_to, frame_id + '_ft.pt'))\n",
    "\n",
    "        xy = ft[:, :2]\n",
    "        # angles = torch.atan(ft[:, 110:]) / 90\n",
    "        # ft = torch.cat([xy, angles], dim=1)\n",
    "        #\n",
    "        return xy\n",
    "\n",
    "    # return ft\n",
    "\n",
    "\n",
    "class Sign_Dataset(Dataset):\n",
    "    def __init__(self, index_file_path, split, pose_root, sample_strategy='rnd_start', num_samples=25, num_copies=4,\n",
    "                 img_transforms=None, video_transforms=None, test_index_file=None):\n",
    "        assert os.path.exists(index_file_path), \"Non-existent indexing file path: {}.\".format(index_file_path)\n",
    "        assert os.path.exists(pose_root), \"Path to poses does not exist: {}.\".format(pose_root)\n",
    "\n",
    "        self.data = []\n",
    "        self.label_encoder, self.onehot_encoder = LabelEncoder(), OneHotEncoder(categories='auto')\n",
    "\n",
    "        if type(split) == 'str':\n",
    "            split = [split]\n",
    "\n",
    "        self.test_index_file = test_index_file\n",
    "        self._make_dataset(index_file_path, split)\n",
    "\n",
    "        self.index_file_path = index_file_path\n",
    "        self.pose_root = pose_root\n",
    "        self.framename = 'image_{}_keypoints.json'\n",
    "        self.sample_strategy = sample_strategy\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        self.img_transforms = img_transforms\n",
    "        self.video_transforms = video_transforms\n",
    "\n",
    "        self.num_copies = num_copies\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(\"lllll\",index)\n",
    "        # print(self.data[index])\n",
    "        # video_id, gloss_cat, frame_start, frame_end = self.data[index]\n",
    "\n",
    "        data_item = self.data[index]\n",
    "        # print(\"data: \",data_item)\n",
    "        video_gloss = data_item['gloss']\n",
    "        video_id = data_item['video_id']\n",
    "        gloss_cat = data_item['gloss_cat']\n",
    "        frame_start = data_item['frame_start']\n",
    "        frame_end = data_item['frame_end']\n",
    "        # # frames of dimensions (T, H, W, C)\n",
    "        # print(\"inside getitem\")\n",
    "        # print(\"Video ID:\", video_id)\n",
    "        # print(\"Frame Start:\", frame_start)\n",
    "        # print(\"Frame End:\", frame_end)\n",
    "        x = self._load_poses(video_id, frame_start, frame_end, self.sample_strategy, self.num_samples)\n",
    "        if self.video_transforms:\n",
    "            x = self.video_transforms(x)\n",
    "\n",
    "        y = gloss_cat\n",
    "\n",
    "        return x, y, video_id,video_gloss\n",
    "    \n",
    "\n",
    "    \n",
    "    def _make_dataset(self, index_file_path, split):\n",
    "        with open(index_file_path, 'r') as f:\n",
    "            content = json.load(f)\n",
    "        # with open(index_file_path, 'r', encoding='utf-8') as f:  # Specify the encoding explicitly\n",
    "        #     content = json.load(f)\n",
    "            \n",
    "\n",
    "        # create label encoder\n",
    "        glosses = sorted([gloss_entry['gloss'] for gloss_entry in content])\n",
    "        # print(glosses)\n",
    "\n",
    "        self.label_encoder.fit(glosses)\n",
    "        self.onehot_encoder.fit(self.label_encoder.transform(self.label_encoder.classes_).reshape(-1, 1))\n",
    "\n",
    "        if self.test_index_file is not None:\n",
    "            print('Trained on {}, tested on {}'.format(index_file_path, self.test_index_file))\n",
    "            with open(self.test_index_file, 'r') as f:\n",
    "                content = json.load(f)\n",
    "\n",
    "        # make dataset\n",
    "            \n",
    "        for gloss_entry in content:\n",
    "            # print(gloss_entry)\n",
    "            gloss, instances = gloss_entry['gloss'], gloss_entry['instances']\n",
    "            # print(instances[0])\n",
    "            \n",
    "            gloss_cat = utils.labels2cat(self.label_encoder, [gloss])[0]\n",
    "\n",
    "            # print(gloss_cat)\n",
    "\n",
    "            for instance in instances:\n",
    "                # print(instance)\n",
    "                if instance['split'] not in split:\n",
    "                    continue\n",
    "                # print('frame;;;;;; ',frame_start)\n",
    "                frame_end = instance['frame_end']\n",
    "                # print(frame_end)\n",
    "                frame_start = instance['frame_start']\n",
    "                video_id = instance['video_id']\n",
    "\n",
    "                instance_entry = {\n",
    "                    'gloss':gloss,\n",
    "                    'video_id': video_id,\n",
    "                    'gloss_cat': gloss_cat,\n",
    "                    'frame_start': frame_start,\n",
    "                    'frame_end': frame_end\n",
    "                }\n",
    "                self.data.append(instance_entry)\n",
    "                # print(self.data)\n",
    "\n",
    "\n",
    "    def _load_poses(self, video_id, frame_start, frame_end, sample_strategy, num_samples):\n",
    "        \"\"\" Load frames of a video. Start and end indices are provided just to avoid listing and sorting the directory unnecessarily.\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"Inside _load_poses:\")\n",
    "        # print(\"Video ID:\", video_id)\n",
    "        # print(\"Frame Start:\", frame_start)\n",
    "        # print(\"Frame End:\", frame_end)\n",
    "        poses = []\n",
    "\n",
    "        # print(\"frame_start: \",frame_start)\n",
    "        frame_start = int(frame_start)  # Convert frame_start to integer\n",
    "        frame_end = int(frame_end)      # Convert frame_end to integer\n",
    "\n",
    "        if sample_strategy == 'rnd_start':\n",
    "            frames_to_sample = rand_start_sampling(frame_start, frame_end, num_samples)\n",
    "        elif sample_strategy == 'seq':\n",
    "            frames_to_sample = sequential_sampling(frame_start, frame_end, num_samples)\n",
    "        elif sample_strategy == 'k_copies':\n",
    "            frames_to_sample = k_copies_fixed_length_sequential_sampling(frame_start, frame_end, num_samples,\n",
    "                                                                     self.num_copies)\n",
    "        else:\n",
    "            raise NotImplementedError('Unimplemented sample strategy found: {}.'.format(sample_strategy))\n",
    "        \n",
    "        # print(\"frame::::\",frames_to_sample)\n",
    "\n",
    "        for i in frames_to_sample:\n",
    "            pose_path = os.path.join(self.pose_root, video_id, self.framename.format(str(i).zfill(5)))\n",
    "            # pose = cv2.imread(frame_path, cv2.COLOR_BGR2RGB)\n",
    "            pose = read_pose_file(pose_path)\n",
    "\n",
    "            # print(\"pose::\",pose)\n",
    "            # print(self.img_transforms)\n",
    "            if pose is not None:\n",
    "                if self.img_transforms:\n",
    "                    # print(self.img_transforms(pose))\n",
    "                    pose = self.img_transforms(pose)\n",
    "\n",
    "                poses.append(pose)\n",
    "            else:\n",
    "                try:\n",
    "                    poses.append(poses[-1])\n",
    "                except IndexError:\n",
    "                    print(pose_path)\n",
    "\n",
    "        pad = None\n",
    "\n",
    "        # if len(frames_to_sample) < num_samples:\n",
    "        if len(poses) < num_samples:\n",
    "            num_padding = num_samples - len(frames_to_sample)\n",
    "            last_pose = poses[-1]\n",
    "            pad = last_pose.repeat(1, num_padding)\n",
    "\n",
    "        # print(\"Poses::::\",poses)\n",
    "\n",
    "        poses_across_time = torch.cat(poses, dim=1)\n",
    "        if pad is not None:\n",
    "            poses_across_time = torch.cat([poses_across_time, pad], dim=1)\n",
    "\n",
    "        return poses_across_time\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# # ---------------commented -------------------------------\n",
    "#     def _make_dataset(self, index_file_path, split):\n",
    "#         with open(index_file_path, 'r') as f:\n",
    "#             content = json.load(f)\n",
    "\n",
    "#         # create label encoder\n",
    "#         glosses = sorted([gloss_entry['gloss'] for gloss_entry in content])\n",
    "\n",
    "#         self.label_encoder.fit(glosses)\n",
    "#         self.onehot_encoder.fit(self.label_encoder.transform(self.label_encoder.classes_).reshape(-1, 1))\n",
    "\n",
    "#         if self.test_index_file is not None:\n",
    "#             print('Trained on {}, tested on {}'.format(index_file_path, self.test_index_file))\n",
    "#             with open(self.test_index_file, 'r') as f:\n",
    "#                 content = json.load(f)\n",
    "\n",
    "#         # make dataset\n",
    "#         for gloss_entry in content:\n",
    "#             gloss, instances = gloss_entry['gloss'], gloss_entry['instances']\n",
    "#             gloss_cat = utils.labels2cat(self.label_encoder, [gloss])[0]\n",
    "\n",
    "#             for instance in instances:\n",
    "#                 if instance['split'] not in split:\n",
    "#                     continue\n",
    "\n",
    "#                 frame_end = instance['frame_end']\n",
    "#                 frame_start = instance['frame_start']\n",
    "#                 video_id = instance['video_id']\n",
    "\n",
    "#                 instance_entry = video_id, gloss_cat, frame_start, frame_end\n",
    "#                 self.data.append(instance_entry)\n",
    "# -----------------------------------------------------------------------\n",
    "                \n",
    "# # -------------commented 2------------------------------------------------------\n",
    "#     def _load_poses(self, video_id, frame_start, frame_end, sample_strategy, num_samples):\n",
    "#         \"\"\" Load frames of a video. Start and end indices are provided just to avoid listing and sorting the directory unnecessarily.\n",
    "#          \"\"\"\n",
    "#         poses = []\n",
    "\n",
    "#         if sample_strategy == 'rnd_start':\n",
    "#             frames_to_sample = rand_start_sampling(frame_start, frame_end, num_samples)\n",
    "#         elif sample_strategy == 'seq':\n",
    "#             frames_to_sample = sequential_sampling(frame_start, frame_end, num_samples)\n",
    "#         elif sample_strategy == 'k_copies':\n",
    "#             frames_to_sample = k_copies_fixed_length_sequential_sampling(frame_start, frame_end, num_samples,\n",
    "#                                                                          self.num_copies)\n",
    "#         else:\n",
    "#             raise NotImplementedError('Unimplemented sample strategy found: {}.'.format(sample_strategy))\n",
    "\n",
    "#         for i in frames_to_sample:\n",
    "#             pose_path = os.path.join(self.pose_root, video_id, self.framename.format(str(i).zfill(5)))\n",
    "#             # pose = cv2.imread(frame_path, cv2.COLOR_BGR2RGB)\n",
    "#             pose = read_pose_file(pose_path)\n",
    "\n",
    "#             if pose is not None:\n",
    "#                 if self.img_transforms:\n",
    "#                     pose = self.img_transforms(pose)\n",
    "\n",
    "#                 poses.append(pose)\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     poses.append(poses[-1])\n",
    "#                 except IndexError:\n",
    "#                     print(pose_path)\n",
    "\n",
    "#         pad = None\n",
    "\n",
    "#         # if len(frames_to_sample) < num_samples:\n",
    "#         if len(poses) < num_samples:\n",
    "#             num_padding = num_samples - len(frames_to_sample)\n",
    "#             last_pose = poses[-1]\n",
    "#             pad = last_pose.repeat(1, num_padding)\n",
    "\n",
    "#         poses_across_time = torch.cat(poses, dim=1)\n",
    "#         if pad is not None:\n",
    "#             poses_across_time = torch.cat([poses_across_time, pad], dim=1)\n",
    "\n",
    "#         return poses_across_time\n",
    "    \n",
    "\n",
    "# # -------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def rand_start_sampling(frame_start, frame_end, num_samples):\n",
    "    \"\"\"Randomly select a starting point and return the continuous ${num_samples} frames.\"\"\"\n",
    "    num_frames = frame_end - frame_start + 1\n",
    "\n",
    "    if num_frames > num_samples:\n",
    "        select_from = range(frame_start, frame_end - num_samples + 1)\n",
    "        sample_start = random.choice(select_from)\n",
    "        frames_to_sample = list(range(sample_start, sample_start + num_samples))\n",
    "    else:\n",
    "        frames_to_sample = list(range(frame_start, frame_end + 1))\n",
    "\n",
    "    return frames_to_sample\n",
    "\n",
    "\n",
    "def sequential_sampling(frame_start, frame_end, num_samples):\n",
    "    \"\"\"Keep sequentially ${num_samples} frames from the whole video sequence by uniformly skipping frames.\"\"\"\n",
    "    num_frames = frame_end - frame_start + 1\n",
    "\n",
    "    frames_to_sample = []\n",
    "    if num_frames > num_samples:\n",
    "        frames_skip = set()\n",
    "\n",
    "        num_skips = num_frames - num_samples\n",
    "        interval = num_frames // num_skips\n",
    "\n",
    "        for i in range(frame_start, frame_end + 1):\n",
    "            if i % interval == 0 and len(frames_skip) <= num_skips:\n",
    "                frames_skip.add(i)\n",
    "\n",
    "        for i in range(frame_start, frame_end + 1):\n",
    "            if i not in frames_skip:\n",
    "                frames_to_sample.append(i)\n",
    "    else:\n",
    "        frames_to_sample = list(range(frame_start, frame_end + 1))\n",
    "\n",
    "    return frames_to_sample\n",
    "\n",
    "\n",
    "def k_copies_fixed_length_sequential_sampling(frame_start, frame_end, num_samples, num_copies):\n",
    "    num_frames = frame_end - frame_start + 1\n",
    "\n",
    "    frames_to_sample = []\n",
    "\n",
    "    if num_frames <= num_samples:\n",
    "        num_pads = num_samples - num_frames\n",
    "\n",
    "        frames_to_sample = list(range(frame_start, frame_end + 1))\n",
    "        frames_to_sample.extend([frame_end] * num_pads)\n",
    "\n",
    "        frames_to_sample *= num_copies\n",
    "\n",
    "    elif num_samples * num_copies < num_frames:\n",
    "        mid = (frame_start + frame_end) // 2\n",
    "        half = num_samples * num_copies // 2\n",
    "\n",
    "        frame_start = mid - half\n",
    "\n",
    "        for i in range(num_copies):\n",
    "            frames_to_sample.extend(list(range(frame_start + i * num_samples,\n",
    "                                               frame_start + i * num_samples + num_samples)))\n",
    "\n",
    "    else:\n",
    "        stride = math.floor((num_frames - num_samples) / (num_copies - 1))\n",
    "        for i in range(num_copies):\n",
    "            frames_to_sample.extend(list(range(frame_start + i * stride,\n",
    "                                               frame_start + i * stride + num_samples)))\n",
    "\n",
    "    return frames_to_sample\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # root = '/home/dxli/workspace/nslt'\n",
    "    #\n",
    "    # split_file = os.path.join(root, 'data/splits-with-dialect-annotated/asl100.json')\n",
    "    # pose_data_root = os.path.join(root, 'data/pose/pose_per_individual_videos')\n",
    "    #\n",
    "    # num_samples = 64\n",
    "    #\n",
    "    # train_dataset = Sign_Dataset(index_file_path=split_file, split=['train', 'val'], pose_root=pose_data_root,\n",
    "    #                              img_transforms=None, video_transforms=None,\n",
    "    #                              num_samples=num_samples)\n",
    "    #\n",
    "    # train_data_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "    #\n",
    "    # cnt = 0\n",
    "    # for batch_idx, data in enumerate(train_data_loader):\n",
    "    #     print(batch_idx)\n",
    "    #     x = data[0]\n",
    "    #     y = data[1]\n",
    "    #     print(x.size())\n",
    "    #     print(y.size())\n",
    "\n",
    "    print(k_copies_fixed_length_sequential_sampling(0, 2, 20, num_copies=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe8092",
   "metadata": {},
   "source": [
    "# model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9ab790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from pygcn.layers import GraphConvolution\n",
    "from layers import GraphConvolution\n",
    "\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174cbe3a",
   "metadata": {},
   "source": [
    "# layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed1e102",
   "metadata": {},
   "source": [
    "# gen feactures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import torch\n",
    "\n",
    "\n",
    "def compute_difference(x):\n",
    "    diff = []\n",
    "\n",
    "    for i, xx in enumerate(x):\n",
    "        temp = []\n",
    "        for j, xxx in enumerate(x):\n",
    "            if i != j:\n",
    "                temp.append(xx - xxx)\n",
    "\n",
    "        diff.append(temp)\n",
    "\n",
    "    return diff\n",
    "\n",
    "\n",
    "def gen(entry_list):\n",
    "    for i, entry in enumerate(entry_list):\n",
    "        for instance in entry['instances']:\n",
    "            vid = instance['video_id']\n",
    "\n",
    "            frame_start = instance['frame_start']\n",
    "            frame_end = instance['frame_end']\n",
    "\n",
    "            save_to = os.path.join('/home/dxli/workspace/nslt/code/Pose-GCN/posegcn/features', vid)\n",
    "\n",
    "            if not os.path.exists(save_to):\n",
    "                os.mkdir(save_to)\n",
    "\n",
    "            for frame_id in range(frame_start, frame_end + 1):\n",
    "                frame_id = 'image_{}'.format(str(frame_id).zfill(5))\n",
    "\n",
    "                ft_path = os.path.join(save_to, frame_id + '_ft.pt')\n",
    "                if not os.path.exists(ft_path):\n",
    "                    try:\n",
    "                        pose_content = json.load(open(os.path.join('/home/dxli/workspace/nslt/data/pose/pose_per_individual_videos',\n",
    "                                                                   vid, frame_id + '_keypoints.json')))[\"people\"][0]\n",
    "                    except IndexError:\n",
    "                        continue\n",
    "\n",
    "                    body_pose = pose_content[\"pose_keypoints_2d\"]\n",
    "                    left_hand_pose = pose_content[\"hand_left_keypoints_2d\"]\n",
    "                    right_hand_pose = pose_content[\"hand_right_keypoints_2d\"]\n",
    "\n",
    "                    body_pose.extend(left_hand_pose)\n",
    "                    body_pose.extend(right_hand_pose)\n",
    "\n",
    "                    x = [v for i, v in enumerate(body_pose) if i % 3 == 0 and i // 3 not in body_pose_exclude]\n",
    "                    y = [v for i, v in enumerate(body_pose) if i % 3 == 1 and i // 3 not in body_pose_exclude]\n",
    "                    # conf = [v for i, v in enumerate(body_pose) if i % 3 == 2 and i // 3 not in body_pose_exclude]\n",
    "\n",
    "                    x = 2 * ((torch.FloatTensor(x) / 256.0) - 0.5)\n",
    "                    y = 2 * ((torch.FloatTensor(y) / 256.0) - 0.5)\n",
    "                    # conf = torch.FloatTensor(conf)\n",
    "\n",
    "                    x_diff = torch.FloatTensor(compute_difference(x)) / 2\n",
    "                    y_diff = torch.FloatTensor(compute_difference(y)) / 2\n",
    "\n",
    "                    zero_indices = (x_diff == 0).nonzero()\n",
    "                    orient = y_diff / x_diff\n",
    "                    orient[zero_indices] = 0\n",
    "\n",
    "                    xy = torch.stack([x, y]).transpose_(0, 1)\n",
    "                    ft = torch.cat([xy, x_diff, y_diff, orient], dim=1)\n",
    "\n",
    "                    torch.save(ft, ft_path)\n",
    "\n",
    "        print('Finish {}-th entry'.format(i))\n",
    "\n",
    "\n",
    "body_pose_exclude = {9, 10, 11, 22, 23, 24, 12, 13, 14, 19, 20, 21}\n",
    "index_file_path = '/home/dxli/workspace/nslt/data/splits-with-dialect-annotated/asl2000.json'\n",
    "\n",
    "with open(index_file_path, 'r') as f:\n",
    "    content = json.load(f)\n",
    "\n",
    "# create label encoder\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "entries_1 = content[0: 700]\n",
    "entries_2 = content[700: 1400]\n",
    "entries_3 = content[1400: ]\n",
    "\n",
    "entry_splits = [entries_1, entries_2, entries_3]\n",
    "\n",
    "p = Pool(3)\n",
    "print(p.map(gen, entry_splits))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99fc684",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e79719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, config_path):\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(config_path)\n",
    "\n",
    "        # training\n",
    "        train_config = config['TRAIN']\n",
    "        self.batch_size = int(train_config['BATCH_SIZE'])\n",
    "        self.max_epochs = int(train_config['MAX_EPOCHS'])\n",
    "        self.log_interval = int(train_config['LOG_INTERVAL'])\n",
    "        self.num_samples = int(train_config['NUM_SAMPLES'])\n",
    "        self.drop_p = float(train_config['DROP_P'])\n",
    "\n",
    "        # optimizer\n",
    "        opt_config = config['OPTIMIZER']\n",
    "        self.init_lr = float(opt_config['INIT_LR'])\n",
    "        self.adam_eps = float(opt_config['ADAM_EPS'])\n",
    "        self.adam_weight_decay = float(opt_config['ADAM_WEIGHT_DECAY'])\n",
    "\n",
    "        # GCN\n",
    "        gcn_config = config['GCN']\n",
    "        self.hidden_size = int(gcn_config['HIDDEN_SIZE'])\n",
    "        self.num_stages = int(gcn_config['NUM_STAGES'])\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'bs={}_ns={}_drop={}_lr={}_eps={}_wd={}'.format(\n",
    "            self.batch_size, self.num_samples, self.drop_p, self.init_lr, self.adam_eps, self.adam_weight_decay\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config_path = '..\\configs\\asl2000.ini'\n",
    "    print(str(Config(config_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288d024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb4a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
